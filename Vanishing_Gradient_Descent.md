- Là hiện tượng khi giá trị của gradient trở nên rất nhỏ (~0) sau một số vòng lặp, gần như không thay đổi
- Vấn đề này trở nên càng tồi tệ khi số lượng layers càng lớn.
- Nguyên nhân là do việc lựa chọn các activation function như sigmoid và tanh. Do các hàm này đưa giá trị tại các neural về khoảng rất nhỏ (vd: sigmoid : [-1,1]); trong khoảng này, ngay cả một thay đổi lớn ở input cũng có thể chỉ làm thay đổi nhỏ ở output.
- Cách giải quyết: Sử dụng Rectified Linear Unit activation function thay cho sigmoid và tanh.
- Ngược lại với Vanishing, exploding gradient là hiện tượng gradient tiến nhanh tới vô cùng.
